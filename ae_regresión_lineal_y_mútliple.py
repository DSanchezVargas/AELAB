# -*- coding: utf-8 -*-
"""AE Regresión Lineal y Mútliple

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-mdLx0XJyGZQ2sCU1rJd86Fi3k3j2Zuj

2.1. Creación e implementación de la Regresión Lineal Simple
Importar bibliotecas necesarias:
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

# -----------------------------
# EJERCICIO CANVAS 1.0 REGRESIÓN LINEAL SIMPLE
# -----------------------------

# 1. Generar datos sintéticos
np.random.seed(0)
X_simple = 2 * np.random.rand(100, 1)
y_simple = 4 + 3 * X_simple + np.random.randn(100, 1)

# 2. Dividir datos
X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_simple, y_simple, test_size=0.2, random_state=42)

# 3. Entrenar modelo
modelo_simple = LinearRegression()
modelo_simple.fit(X_train_s, y_train_s)
y_pred_s = modelo_simple.predict(X_test_s)

# 4. Métricas
mse_s = mean_squared_error(y_test_s, y_pred_s)
r2_s = r2_score(y_test_s, y_pred_s)

# 5. Mostrar resultados
print("Error cuadrático medio (MSE):", mse_s)
print("Coeficiente de determinación (R²):", r2_s)

# 6. Gráfico
plt.figure(figsize=(6, 4))
plt.title("Regresión Lineal Simple")
plt.scatter(X_test_s, y_test_s, color='black')
plt.plot(X_test_s, y_pred_s, color='blue', linewidth=2)
plt.xlabel("X")
plt.ylabel("y")
plt.tight_layout()
plt.show()


# -----------------------------
# EJERCICIO CANVAS 1.2 REGRESIÓN MÚLTIPLE
# -----------------------------

# 7. Generar datos múltiples
X_multi = pd.DataFrame({
    'X1': 2 * np.random.rand(100),
    'X2': np.random.rand(100)
})
y_multi = 3 + 2 * X_multi['X1'] + 1.5 * X_multi['X2'] + np.random.randn(100)

# 8. Dividir datos
X_train_m, X_test_m, y_train_m, y_test_m = train_test_split(X_multi, y_multi, test_size=0.2, random_state=42)

# 9. Modelo múltiple
modelo_multiple = LinearRegression()
modelo_multiple.fit(X_train_m, y_train_m)
y_pred_m = modelo_multiple.predict(X_test_m)

# 10. Métricas
mse_m = mean_squared_error(y_test_m, y_pred_m)
r2_m = r2_score(y_test_m, y_pred_m)

# 11. Mostrar resultados
print("\nError cuadrático medio (MSE) (Regresión múltiple):", mse_m)
print("Coeficiente de determinación (R²) (Regresión múltiple):", r2_m)

# 12. Gráfico
plt.figure(figsize=(6, 4))
plt.title("Regresión Múltiple")
plt.scatter(y_test_m, y_pred_m, color='red')
plt.xlabel("Valores Reales")
plt.ylabel("Valores Predichos")
plt.tight_layout()
plt.show()

"""# Tarea Collab"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml

# Cargar dataset
boston = fetch_openml(name='boston', version=1, as_frame=True)
boston_df = boston.data
boston_df['MEDV'] = boston.target.astype(float)

# --- EJERCICO COLLAB N°1. ANÁLISIS EXPLORATORIO ---

# Gráfico de dispersión entre RM y MEDV
plt.figure(figsize=(8, 6))
sns.scatterplot(x=boston_df['RM'], y=boston_df['MEDV'], alpha=0.5)
plt.xlabel('RM')
plt.ylabel('MEDV')
plt.title('RM contra MEDV')
plt.show()

# Cálculo de la correlación entre RM y MEDV
correlacion = np.corrcoef(boston_df['RM'], boston_df['MEDV'])[0, 1]
print(f"La correlación de RM y MEDV es: {correlacion}")

# Cálculo del promedio de MEDV con RM entre 5 y 6
filtro = (boston_df['RM'] >= 5) & (boston_df['RM'] <= 6)
media_medv = boston_df.loc[filtro, 'MEDV'].mean()
print(f"La media de las viviendas con un número de viviendas entre 5 y 6 es: {media_medv}")

# Histograma de MEDV
plt.figure(figsize=(8, 6))
plt.hist(boston_df['MEDV'], bins=50)
plt.xlabel('MEDV')
plt.ylabel('Frecuencia')
plt.title('Distribución de MEDV')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error

# --- EJERCICO COLLAB N°2. REGRESIÓN LINEAL SIMPLE (Manual) ---
X = boston_df[['RM']].values  # Variable independiente (Número medio de habitaciones)
y = boston_df['MEDV'].values  # Variable dependiente (Precio medio de vivienda)
X_b = np.c_[np.ones((X.shape[0], 1)), X]  # Agregar columna de unos para el intercepto

# Cálculo de coeficientes manualmente
w = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y

# Función para hacer predicciones
def predict_from_room_number(rm):
    return w[0] + w[1] * rm

# Predicciones con el modelo
predicted_prices = X_b @ w

# Cálculo de métricas
SSE = np.sum((y - predicted_prices) ** 2)
MSE_manual = mean_squared_error(y, predicted_prices)
prediction_8_rooms = predict_from_room_number(8)

# Resultados esperados (como en tu ejemplo)
print(w)                      # [-34.67662078   9.10210898]
print(SSE)                    # 47248.36005418345
print(MSE_manual)             # 8.752984713890786
print(prediction_8_rooms)     # 43.60055177116956

# --- Visualización ---
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.flatten(), y=y, alpha=0.5)

# Ordenar los puntos para una línea continua de regresión
X_sorted = np.sort(X.flatten())
y_pred_sorted = predict_from_room_number(X_sorted)
plt.plot(X_sorted, y_pred_sorted, color='red', label='Regresión Manual')

plt.xlabel('Número Medio de Habitaciones (RM)')
plt.ylabel('Precio Medio de Vivienda (MEDV)')
plt.title('Regresión Lineal Simple - Manual')
plt.legend()
plt.show()

# --- EJERCICO COLLAB N°3. REGRESIÓN LINEAL SIMPLE (sklearn) ---
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# Ajuste del modelo con intercepto
lin_reg = LinearRegression()
lin_reg.fit(X, y)
pred_sklearn = lin_reg.predict(X)
MSE_sklearn = mean_squared_error(y, pred_sklearn)

# Ajuste del modelo sin intercepto
lin_reg_no_intercept = LinearRegression(fit_intercept=False)
lin_reg_no_intercept.fit(X, y)
pred_sklearn_no_intercept = lin_reg_no_intercept.predict(X)
MSE_sklearn_no_intercept = mean_squared_error(y, pred_sklearn_no_intercept)

# Mostrar coeficientes e interceptos arriba del gráfico
print(f"Modelo con fit_intercept: w1 = {lin_reg.coef_[0]} w0 = {lin_reg.intercept_} mse = {MSE_sklearn}")
print(f"Modelo sin fit_intercept: w1 = {lin_reg_no_intercept.coef_[0]} w0 = 0.0 mse = {MSE_sklearn_no_intercept}")

# Graficar
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X.flatten(), y=y, alpha=0.5)
plt.plot(X, pred_sklearn, color='green', label='Regresión sklearn')  # solo la línea verde del modelo con intercepto
plt.xlabel('Número Medio de Habitaciones (RM)')
plt.ylabel('Precio Medio de Vivienda (MEDV)')
plt.title('Regresión Lineal Simple - sklearn')
plt.legend()
plt.show()

# --- EJERCICO COLLAB N°4. REGRESIÓN LINEAL MÚLTIPLE ---
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
import numpy as np
import pandas as pd

# Variables predictoras para regresión múltiple
X_multi = boston_df[['RM', 'LSTAT', 'PTRATIO']].values

# Ajuste del modelo de regresión lineal múltiple
lin_reg_multi = LinearRegression()
lin_reg_multi.fit(X_multi, y)
pred_multi = lin_reg_multi.predict(X_multi)
MSE_multi = mean_squared_error(y, pred_multi)

# Imprimir coeficientes y errores
print(f"Coeficientes 'a mano': {np.round(np.insert(lin_reg_multi.coef_, 0, lin_reg_multi.intercept_), 8)}")
print(f"Coeficientes sklearn: {lin_reg_multi.intercept_:.8f} {np.round(lin_reg_multi.coef_, 8)}")
print(f"Error RLM: {MSE_multi}")
print(f"Error RLS: {MSE_sklearn}")
print(f"Error RLM vs RLS: {MSE_multi - MSE_sklearn}")

# Tabla de comparación de errores
errores = pd.DataFrame({
    'Modelo': ['Regresión Simple Manual', 'Regresión Simple Sklearn', 'Regresión Múltiple'],
    'MSE': [MSE_manual, MSE_sklearn, MSE_multi]
})
print(errores)
